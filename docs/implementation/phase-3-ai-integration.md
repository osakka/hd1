# Phase 3: AI Integration Implementation Plan
**Duration**: 2 months  
**Goal**: Transform HD1 into AI-native platform with intelligent 3D avatars  
**Endpoints**: 60 â†’ 80

## Overview
Phase 3 integrates artificial intelligence as a first-class citizen in the universal 3D interface platform. We implement LLM avatars with visual understanding, AI-generated 3D content, natural language interaction, and comprehensive analytics to create an intelligent, responsive 3D environment.

## Technical Objectives

### 1. LLM Avatar System
**Current State**: Basic 3D avatars without intelligence  
**Target State**: Intelligent avatars with visual understanding and natural language interaction

**Implementation Steps**:
1. **LLM Integration Framework**
   - OpenAI API integration with custom prompts
   - Context management for 3D scene understanding
   - Memory and personality persistence
   - Multi-modal input processing (text, voice, visual)

2. **Avatar Intelligence Engine**
   - Visual scene analysis and object recognition
   - Spatial reasoning and navigation
   - Natural language command interpretation
   - Emotional and personality modeling

3. **Avatar Behavior System**
   - Autonomous movement and interaction
   - Goal-oriented task execution
   - Social interaction with users and other avatars
   - Learning and adaptation mechanisms

### 2. Computer Vision Integration
**Current State**: No visual understanding  
**Target State**: Complete 3D scene analysis and object recognition

**Implementation Steps**:
1. **3D Scene Analysis**
   - Three.js scene graph parsing
   - Object detection and classification
   - Spatial relationship mapping
   - Dynamic scene change detection

2. **Visual Understanding API**
   - Scene description generation
   - Object interaction suggestions
   - Visual question answering
   - Accessibility features for visually impaired users

### 3. AI-Generated Content Pipeline
**Current State**: Manual 3D content creation  
**Target State**: AI-powered 3D content generation from natural language

**Implementation Steps**:
1. **Content Generation Framework**
   - Text-to-3D model generation
   - Procedural scene creation
   - Dynamic texture generation
   - Animation and behavior scripting

2. **Quality Assurance System**
   - Generated content validation
   - Performance optimization
   - Style consistency checking
   - User feedback integration

### 4. Natural Language Interface
**Current State**: API-based interaction only  
**Target State**: Natural language control of 3D environment

**Implementation Steps**:
1. **Command Interpretation**
   - Natural language parsing
   - Intent recognition and slot filling
   - Context-aware command processing
   - Multi-step task execution

2. **Conversational Interface**
   - Voice-to-text integration
   - Text-to-speech synthesis
   - Dialog management
   - Personality-based responses

### 5. Analytics and Monitoring
**Current State**: Basic logging  
**Target State**: Comprehensive analytics with AI insights

**Implementation Steps**:
1. **User Behavior Analytics**
   - Interaction pattern analysis
   - Engagement metrics
   - Performance monitoring
   - Predictive analytics

2. **AI Performance Monitoring**
   - LLM response quality tracking
   - Avatar behavior analysis
   - Content generation metrics
   - System performance optimization

## Detailed Implementation

### Step 1: LLM Avatar Framework
```go
// src/llm/avatar.go
package llm

import (
    "context"
    "encoding/json"
    "fmt"
    "time"
    
    "github.com/google/uuid"
    "holodeck1/database"
    "holodeck1/logging"
    "holodeck1/openai"
)

type LLMAvatar struct {
    ID           uuid.UUID              `json:"id"`
    Name         string                 `json:"name"`
    LLMModel     string                 `json:"llm_model"`
    Personality  string                 `json:"personality"`
    Capabilities []string               `json:"capabilities"`
    VisualModel  string                 `json:"visual_model"`
    VoiceModel   string                 `json:"voice_model"`
    Position     Vector3                `json:"position"`
    SessionID    uuid.UUID              `json:"session_id"`
    CreatedAt    time.Time              `json:"created_at"`
    UpdatedAt    time.Time              `json:"updated_at"`
    Memory       map[string]interface{} `json:"memory"`
    Context      *SceneContext          `json:"context"`
}

type SceneContext struct {
    Objects       []SceneObject          `json:"objects"`
    Users         []User                 `json:"users"`
    Interactions  []Interaction          `json:"interactions"`
    Environment   map[string]interface{} `json:"environment"`
    Timestamp     time.Time              `json:"timestamp"`
}

type LLMAvatarManager struct {
    db        *database.DB
    openai    *openai.Client
    vision    *ComputerVision
    speech    *SpeechSynthesis
    avatars   map[uuid.UUID]*LLMAvatar
}

func NewLLMAvatarManager(db *database.DB, openaiClient *openai.Client, vision *ComputerVision, speech *SpeechSynthesis) *LLMAvatarManager {
    return &LLMAvatarManager{
        db:      db,
        openai:  openaiClient,
        vision:  vision,
        speech:  speech,
        avatars: make(map[uuid.UUID]*LLMAvatar),
    }
}

func (lam *LLMAvatarManager) CreateLLMAvatar(ctx context.Context, req *CreateLLMAvatarRequest) (*LLMAvatar, error) {
    avatar := &LLMAvatar{
        ID:           uuid.New(),
        Name:         req.Name,
        LLMModel:     req.LLMModel,
        Personality:  req.Personality,
        Capabilities: req.Capabilities,
        VisualModel:  req.VisualModel,
        VoiceModel:   req.VoiceModel,
        Position:     req.Position,
        SessionID:    req.SessionID,
        CreatedAt:    time.Now(),
        UpdatedAt:    time.Now(),
        Memory:       make(map[string]interface{}),
    }
    
    // Initialize scene context
    context, err := lam.buildSceneContext(ctx, req.SessionID)
    if err != nil {
        return nil, err
    }
    avatar.Context = context
    
    // Save to database
    err = lam.db.CreateLLMAvatar(ctx, avatar)
    if err != nil {
        logging.Error("failed to create LLM avatar", map[string]interface{}{
            "avatar_id": avatar.ID,
            "error": err.Error(),
        })
        return nil, err
    }
    
    // Store in memory
    lam.avatars[avatar.ID] = avatar
    
    // Initialize avatar with personality prompt
    err = lam.initializeAvatar(ctx, avatar)
    if err != nil {
        logging.Error("failed to initialize avatar", map[string]interface{}{
            "avatar_id": avatar.ID,
            "error": err.Error(),
        })
        return nil, err
    }
    
    return avatar, nil
}

func (lam *LLMAvatarManager) ProcessInteraction(ctx context.Context, avatarID uuid.UUID, interaction *Interaction) (*InteractionResponse, error) {
    avatar, exists := lam.avatars[avatarID]
    if !exists {
        return nil, fmt.Errorf("avatar not found: %s", avatarID)
    }
    
    // Update scene context
    err := lam.updateSceneContext(ctx, avatar)
    if err != nil {
        logging.Error("failed to update scene context", map[string]interface{}{
            "avatar_id": avatarID,
            "error": err.Error(),
        })
    }
    
    // Process interaction based on type
    switch interaction.Type {
    case "text":
        return lam.processTextInteraction(ctx, avatar, interaction)
    case "voice":
        return lam.processVoiceInteraction(ctx, avatar, interaction)
    case "gesture":
        return lam.processGestureInteraction(ctx, avatar, interaction)
    case "visual":
        return lam.processVisualInteraction(ctx, avatar, interaction)
    default:
        return nil, fmt.Errorf("unsupported interaction type: %s", interaction.Type)
    }
}

func (lam *LLMAvatarManager) processTextInteraction(ctx context.Context, avatar *LLMAvatar, interaction *Interaction) (*InteractionResponse, error) {
    // Build prompt with context
    prompt := lam.buildPrompt(avatar, interaction)
    
    // Call OpenAI API
    response, err := lam.openai.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
        Model: avatar.LLMModel,
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    "system",
                Content: prompt.System,
            },
            {
                Role:    "user",
                Content: interaction.Content,
            },
        },
        Temperature: 0.7,
        MaxTokens:   500,
    })
    
    if err != nil {
        logging.Error("failed to get LLM response", map[string]interface{}{
            "avatar_id": avatar.ID,
            "error": err.Error(),
        })
        return nil, err
    }
    
    // Parse response and extract actions
    llmResponse := response.Choices[0].Message.Content
    actions, err := lam.parseActionsFromResponse(llmResponse)
    if err != nil {
        logging.Error("failed to parse actions from LLM response", map[string]interface{}{
            "avatar_id": avatar.ID,
            "response": llmResponse,
            "error": err.Error(),
        })
    }
    
    // Update avatar memory
    lam.updateAvatarMemory(avatar, interaction, llmResponse)
    
    // Generate speech if voice model is available
    var audioURL string
    if avatar.VoiceModel != "" {
        audioURL, err = lam.speech.TextToSpeech(ctx, llmResponse, avatar.VoiceModel)
        if err != nil {
            logging.Error("failed to generate speech", map[string]interface{}{
                "avatar_id": avatar.ID,
                "error": err.Error(),
            })
        }
    }
    
    return &InteractionResponse{
        Type:        "text",
        Content:     llmResponse,
        Actions:     actions,
        AudioURL:    audioURL,
        Timestamp:   time.Now(),
    }, nil
}

func (lam *LLMAvatarManager) buildPrompt(avatar *LLMAvatar, interaction *Interaction) *Prompt {
    sceneDescription := lam.generateSceneDescription(avatar.Context)
    
    systemPrompt := fmt.Sprintf(`You are %s, an intelligent 3D avatar in a virtual environment.

Personality: %s

Current Scene: %s

Capabilities: %s

Your goal is to help users interact with the 3D environment naturally. You can:
- Answer questions about objects in the scene
- Help manipulate 3D objects
- Provide guidance and assistance
- Engage in natural conversation

Respond in character and consider the spatial context when providing answers.
If you need to perform actions, include them in your response using the format: [ACTION: action_name(parameters)]

Available actions:
- MOVE(x, y, z) - Move to position
- LOOK_AT(object_id) - Look at specific object
- POINT_TO(object_id) - Point to specific object
- CREATE_OBJECT(type, position) - Create new object
- MODIFY_OBJECT(object_id, property, value) - Modify object property
- SPEAK(text) - Speak text aloud
- GESTURE(gesture_name) - Perform gesture

Memory: %s`,
        avatar.Name,
        avatar.Personality,
        sceneDescription,
        fmt.Sprintf("%v", avatar.Capabilities),
        lam.formatMemory(avatar.Memory),
    )
    
    return &Prompt{
        System: systemPrompt,
        User:   interaction.Content,
    }
}

func (lam *LLMAvatarManager) generateSceneDescription(context *SceneContext) string {
    description := "Scene contains:\n"
    
    for _, obj := range context.Objects {
        description += fmt.Sprintf("- %s at position (%.2f, %.2f, %.2f)\n",
            obj.Type, obj.Position.X, obj.Position.Y, obj.Position.Z)
    }
    
    for _, user := range context.Users {
        description += fmt.Sprintf("- User %s at position (%.2f, %.2f, %.2f)\n",
            user.Name, user.Position.X, user.Position.Y, user.Position.Z)
    }
    
    return description
}

func (lam *LLMAvatarManager) parseActionsFromResponse(response string) ([]Action, error) {
    actions := make([]Action, 0)
    
    // Simple regex-based action parsing
    actionPattern := `\[ACTION: ([^]]+)\]`
    re := regexp.MustCompile(actionPattern)
    matches := re.FindAllStringSubmatch(response, -1)
    
    for _, match := range matches {
        if len(match) > 1 {
            action, err := lam.parseAction(match[1])
            if err != nil {
                logging.Error("failed to parse action", map[string]interface{}{
                    "action_string": match[1],
                    "error": err.Error(),
                })
                continue
            }
            actions = append(actions, action)
        }
    }
    
    return actions, nil
}

func (lam *LLMAvatarManager) ExecuteAction(ctx context.Context, avatarID uuid.UUID, action Action) error {
    avatar, exists := lam.avatars[avatarID]
    if !exists {
        return fmt.Errorf("avatar not found: %s", avatarID)
    }
    
    switch action.Type {
    case "MOVE":
        return lam.executeMove(ctx, avatar, action)
    case "LOOK_AT":
        return lam.executeLookAt(ctx, avatar, action)
    case "POINT_TO":
        return lam.executePointTo(ctx, avatar, action)
    case "CREATE_OBJECT":
        return lam.executeCreateObject(ctx, avatar, action)
    case "MODIFY_OBJECT":
        return lam.executeModifyObject(ctx, avatar, action)
    case "SPEAK":
        return lam.executeSpeak(ctx, avatar, action)
    case "GESTURE":
        return lam.executeGesture(ctx, avatar, action)
    default:
        return fmt.Errorf("unsupported action type: %s", action.Type)
    }
}

func (lam *LLMAvatarManager) executeMove(ctx context.Context, avatar *LLMAvatar, action Action) error {
    // Parse position from action parameters
    x, ok := action.Parameters["x"].(float64)
    if !ok {
        return fmt.Errorf("invalid x coordinate")
    }
    y, ok := action.Parameters["y"].(float64)
    if !ok {
        return fmt.Errorf("invalid y coordinate")
    }
    z, ok := action.Parameters["z"].(float64)
    if !ok {
        return fmt.Errorf("invalid z coordinate")
    }
    
    // Update avatar position
    avatar.Position = Vector3{X: x, Y: y, Z: z}
    avatar.UpdatedAt = time.Now()
    
    // Update in database
    err := lam.db.UpdateLLMAvatar(ctx, avatar)
    if err != nil {
        logging.Error("failed to update avatar position", map[string]interface{}{
            "avatar_id": avatar.ID,
            "error": err.Error(),
        })
        return err
    }
    
    // Broadcast position update via WebSocket
    update := map[string]interface{}{
        "type":      "avatar_move",
        "avatar_id": avatar.ID,
        "position":  avatar.Position,
        "timestamp": time.Now(),
    }
    
    // TODO: Broadcast to session via WebSocket hub
    
    return nil
}
```

### Step 2: Computer Vision Integration
```go
// src/vision/analyzer.go
package vision

import (
    "context"
    "encoding/json"
    "fmt"
    
    "github.com/google/uuid"
    "holodeck1/logging"
    "holodeck1/openai"
)

type ComputerVision struct {
    openai *openai.Client
}

type SceneAnalysis struct {
    Objects       []DetectedObject       `json:"objects"`
    Relationships []SpatialRelationship  `json:"relationships"`
    Description   string                 `json:"description"`
    Suggestions   []InteractionSuggestion `json:"suggestions"`
    Timestamp     time.Time              `json:"timestamp"`
}

type DetectedObject struct {
    ID          uuid.UUID              `json:"id"`
    Type        string                 `json:"type"`
    Position    Vector3                `json:"position"`
    Bounds      BoundingBox            `json:"bounds"`
    Properties  map[string]interface{} `json:"properties"`
    Confidence  float64                `json:"confidence"`
}

type SpatialRelationship struct {
    Object1     uuid.UUID `json:"object1"`
    Object2     uuid.UUID `json:"object2"`
    Relationship string   `json:"relationship"` // "near", "above", "inside", etc.
    Distance    float64   `json:"distance"`
    Confidence  float64   `json:"confidence"`
}

type InteractionSuggestion struct {
    Type        string                 `json:"type"`
    Description string                 `json:"description"`
    Objects     []uuid.UUID            `json:"objects"`
    Actions     []string               `json:"actions"`
    Confidence  float64                `json:"confidence"`
}

func NewComputerVision(openaiClient *openai.Client) *ComputerVision {
    return &ComputerVision{
        openai: openaiClient,
    }
}

func (cv *ComputerVision) AnalyzeScene(ctx context.Context, sceneData *SceneData) (*SceneAnalysis, error) {
    // Extract objects from Three.js scene
    objects, err := cv.extractObjects(sceneData)
    if err != nil {
        return nil, err
    }
    
    // Analyze spatial relationships
    relationships, err := cv.analyzeSpatialRelationships(objects)
    if err != nil {
        return nil, err
    }
    
    // Generate scene description using GPT-4V
    description, err := cv.generateSceneDescription(ctx, objects, relationships)
    if err != nil {
        return nil, err
    }
    
    // Generate interaction suggestions
    suggestions, err := cv.generateInteractionSuggestions(ctx, objects, relationships)
    if err != nil {
        return nil, err
    }
    
    analysis := &SceneAnalysis{
        Objects:       objects,
        Relationships: relationships,
        Description:   description,
        Suggestions:   suggestions,
        Timestamp:     time.Now(),
    }
    
    return analysis, nil
}

func (cv *ComputerVision) extractObjects(sceneData *SceneData) ([]DetectedObject, error) {
    objects := make([]DetectedObject, 0)
    
    for _, entity := range sceneData.Entities {
        object := DetectedObject{
            ID:       entity.ID,
            Type:     cv.classifyObject(entity),
            Position: entity.Position,
            Bounds:   cv.calculateBounds(entity),
            Properties: map[string]interface{}{
                "material": entity.Material,
                "geometry": entity.Geometry,
                "scale":    entity.Scale,
                "rotation": entity.Rotation,
                "visible":  entity.Visible,
            },
            Confidence: 0.95, // High confidence for explicitly created objects
        }
        
        objects = append(objects, object)
    }
    
    return objects, nil
}

func (cv *ComputerVision) analyzeSpatialRelationships(objects []DetectedObject) ([]SpatialRelationship, error) {
    relationships := make([]SpatialRelationship, 0)
    
    for i, obj1 := range objects {
        for j, obj2 := range objects {
            if i != j {
                relationship := cv.determineSpatialRelationship(obj1, obj2)
                if relationship != "" {
                    rel := SpatialRelationship{
                        Object1:      obj1.ID,
                        Object2:      obj2.ID,
                        Relationship: relationship,
                        Distance:     cv.calculateDistance(obj1.Position, obj2.Position),
                        Confidence:   0.85,
                    }
                    relationships = append(relationships, rel)
                }
            }
        }
    }
    
    return relationships, nil
}

func (cv *ComputerVision) generateSceneDescription(ctx context.Context, objects []DetectedObject, relationships []SpatialRelationship) (string, error) {
    // Build scene context for GPT-4V
    sceneContext := map[string]interface{}{
        "objects":       objects,
        "relationships": relationships,
    }
    
    contextJSON, err := json.Marshal(sceneContext)
    if err != nil {
        return "", err
    }
    
    prompt := fmt.Sprintf(`Analyze this 3D scene and provide a natural language description:

Scene Data: %s

Provide a clear, concise description that includes:
1. What objects are present
2. How they are positioned relative to each other
3. Any notable spatial arrangements
4. Potential uses or purposes of the scene

Focus on spatial relationships and practical descriptions that would help someone understand the 3D environment.`, string(contextJSON))
    
    response, err := cv.openai.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
        Model: "gpt-4-vision-preview",
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    "user",
                Content: prompt,
            },
        },
        Temperature: 0.3,
        MaxTokens:   300,
    })
    
    if err != nil {
        logging.Error("failed to generate scene description", map[string]interface{}{
            "error": err.Error(),
        })
        return "", err
    }
    
    return response.Choices[0].Message.Content, nil
}

func (cv *ComputerVision) AnswerVisualQuestion(ctx context.Context, question string, sceneData *SceneData) (string, error) {
    // Analyze scene
    analysis, err := cv.AnalyzeScene(ctx, sceneData)
    if err != nil {
        return "", err
    }
    
    // Build context for question answering
    contextJSON, err := json.Marshal(analysis)
    if err != nil {
        return "", err
    }
    
    prompt := fmt.Sprintf(`Answer this question about a 3D scene:

Question: %s

Scene Analysis: %s

Provide a clear, accurate answer based on the scene data. If the question cannot be answered from the available information, explain what information is missing.`, question, string(contextJSON))
    
    response, err := cv.openai.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
        Model: "gpt-4-vision-preview",
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    "user",
                Content: prompt,
            },
        },
        Temperature: 0.3,
        MaxTokens:   200,
    })
    
    if err != nil {
        logging.Error("failed to answer visual question", map[string]interface{}{
            "question": question,
            "error": err.Error(),
        })
        return "", err
    }
    
    return response.Choices[0].Message.Content, nil
}
```

### Step 3: AI Content Generation
```go
// src/generation/content.go
package generation

import (
    "context"
    "encoding/json"
    "fmt"
    "time"
    
    "github.com/google/uuid"
    "holodeck1/logging"
    "holodeck1/openai"
)

type ContentGenerator struct {
    openai *openai.Client
}

type GenerationRequest struct {
    Type        string                 `json:"type"`
    Description string                 `json:"description"`
    Style       string                 `json:"style"`
    Parameters  map[string]interface{} `json:"parameters"`
    Context     *SceneContext          `json:"context"`
}

type GenerationResult struct {
    ID          uuid.UUID              `json:"id"`
    Type        string                 `json:"type"`
    Content     interface{}            `json:"content"`
    Metadata    map[string]interface{} `json:"metadata"`
    Quality     float64                `json:"quality"`
    CreatedAt   time.Time              `json:"created_at"`
}

func NewContentGenerator(openaiClient *openai.Client) *ContentGenerator {
    return &ContentGenerator{
        openai: openaiClient,
    }
}

func (cg *ContentGenerator) GenerateContent(ctx context.Context, req *GenerationRequest) (*GenerationResult, error) {
    switch req.Type {
    case "object":
        return cg.generateObject(ctx, req)
    case "scene":
        return cg.generateScene(ctx, req)
    case "animation":
        return cg.generateAnimation(ctx, req)
    case "material":
        return cg.generateMaterial(ctx, req)
    case "texture":
        return cg.generateTexture(ctx, req)
    default:
        return nil, fmt.Errorf("unsupported generation type: %s", req.Type)
    }
}

func (cg *ContentGenerator) generateObject(ctx context.Context, req *GenerationRequest) (*GenerationResult, error) {
    prompt := fmt.Sprintf(`Generate a Three.js object specification based on this description:

Description: %s
Style: %s
Context: %s

Generate a JSON object with the following structure:
{
  "geometry": {
    "type": "box|sphere|cylinder|plane|custom",
    "width": number,
    "height": number,
    "depth": number,
    "radius": number,
    "segments": number
  },
  "material": {
    "type": "basic|phong|standard",
    "color": "#hexcode",
    "transparent": boolean,
    "opacity": number,
    "metalness": number,
    "roughness": number
  },
  "position": {"x": number, "y": number, "z": number},
  "rotation": {"x": number, "y": number, "z": number},
  "scale": {"x": number, "y": number, "z": number},
  "animations": [
    {
      "name": "string",
      "type": "rotation|translation|scale",
      "duration": number,
      "loop": boolean,
      "parameters": {}
    }
  ]
}

Make sure the generated object is realistic, properly proportioned, and fits the description and style.`,
        req.Description,
        req.Style,
        cg.formatContext(req.Context),
    )
    
    response, err := cg.openai.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
        Model: "gpt-4",
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    "user",
                Content: prompt,
            },
        },
        Temperature: 0.7,
        MaxTokens:   1000,
    })
    
    if err != nil {
        logging.Error("failed to generate object", map[string]interface{}{
            "description": req.Description,
            "error": err.Error(),
        })
        return nil, err
    }
    
    // Parse generated content
    var objectSpec ObjectSpecification
    err = json.Unmarshal([]byte(response.Choices[0].Message.Content), &objectSpec)
    if err != nil {
        logging.Error("failed to parse generated object specification", map[string]interface{}{
            "response": response.Choices[0].Message.Content,
            "error": err.Error(),
        })
        return nil, err
    }
    
    // Validate and optimize object
    quality, err := cg.validateObject(&objectSpec)
    if err != nil {
        logging.Error("failed to validate generated object", map[string]interface{}{
            "error": err.Error(),
        })
        return nil, err
    }
    
    result := &GenerationResult{
        ID:      uuid.New(),
        Type:    "object",
        Content: &objectSpec,
        Metadata: map[string]interface{}{
            "description": req.Description,
            "style":       req.Style,
            "generated_at": time.Now(),
        },
        Quality:   quality,
        CreatedAt: time.Now(),
    }
    
    return result, nil
}

func (cg *ContentGenerator) generateScene(ctx context.Context, req *GenerationRequest) (*GenerationResult, error) {
    prompt := fmt.Sprintf(`Generate a complete 3D scene specification based on this description:

Description: %s
Style: %s

Generate a JSON scene with the following structure:
{
  "background": "#hexcode",
  "fog": {
    "color": "#hexcode",
    "near": number,
    "far": number
  },
  "lighting": [
    {
      "type": "ambient|directional|point|spot",
      "color": "#hexcode",
      "intensity": number,
      "position": {"x": number, "y": number, "z": number},
      "direction": {"x": number, "y": number, "z": number}
    }
  ],
  "objects": [
    {
      "name": "string",
      "geometry": {},
      "material": {},
      "position": {"x": number, "y": number, "z": number},
      "rotation": {"x": number, "y": number, "z": number},
      "scale": {"x": number, "y": number, "z": number}
    }
  ],
  "environment": {
    "skybox": "url_or_color",
    "ground": {
      "type": "plane|custom",
      "material": {},
      "size": number
    }
  }
}

Create a cohesive, well-designed scene with proper lighting, appropriate object placement, and good visual composition.`,
        req.Description,
        req.Style,
    )
    
    response, err := cg.openai.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
        Model: "gpt-4",
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    "user",
                Content: prompt,
            },
        },
        Temperature: 0.7,
        MaxTokens:   2000,
    })
    
    if err != nil {
        logging.Error("failed to generate scene", map[string]interface{}{
            "description": req.Description,
            "error": err.Error(),
        })
        return nil, err
    }
    
    // Parse generated content
    var sceneSpec SceneSpecification
    err = json.Unmarshal([]byte(response.Choices[0].Message.Content), &sceneSpec)
    if err != nil {
        logging.Error("failed to parse generated scene specification", map[string]interface{}{
            "response": response.Choices[0].Message.Content,
            "error": err.Error(),
        })
        return nil, err
    }
    
    // Validate and optimize scene
    quality, err := cg.validateScene(&sceneSpec)
    if err != nil {
        logging.Error("failed to validate generated scene", map[string]interface{}{
            "error": err.Error(),
        })
        return nil, err
    }
    
    result := &GenerationResult{
        ID:      uuid.New(),
        Type:    "scene",
        Content: &sceneSpec,
        Metadata: map[string]interface{}{
            "description": req.Description,
            "style":       req.Style,
            "generated_at": time.Now(),
        },
        Quality:   quality,
        CreatedAt: time.Now(),
    }
    
    return result, nil
}

func (cg *ContentGenerator) validateObject(spec *ObjectSpecification) (float64, error) {
    quality := 1.0
    
    // Check geometry validity
    if spec.Geometry.Type == "" {
        quality -= 0.3
    }
    
    // Check material validity
    if spec.Material.Type == "" {
        quality -= 0.2
    }
    
    // Check if color is valid hex
    if !isValidHexColor(spec.Material.Color) {
        quality -= 0.1
    }
    
    // Check position reasonableness
    if spec.Position.X < -1000 || spec.Position.X > 1000 ||
       spec.Position.Y < -1000 || spec.Position.Y > 1000 ||
       spec.Position.Z < -1000 || spec.Position.Z > 1000 {
        quality -= 0.2
    }
    
    // Check scale reasonableness
    if spec.Scale.X <= 0 || spec.Scale.Y <= 0 || spec.Scale.Z <= 0 {
        quality -= 0.2
    }
    
    return quality, nil
}
```

### Step 4: Analytics Implementation
```go
// src/analytics/manager.go
package analytics

import (
    "context"
    "time"
    
    "github.com/google/uuid"
    "holodeck1/database"
    "holodeck1/logging"
)

type AnalyticsManager struct {
    db *database.DB
}

type SessionAnalytics struct {
    SessionID           uuid.UUID              `json:"session_id"`
    Duration            time.Duration          `json:"duration"`
    ParticipantCount    int                    `json:"participant_count"`
    InteractionCount    int                    `json:"interaction_count"`
    ServicesUsed        []string               `json:"services_used"`
    LLMInteractions     int                    `json:"llm_interactions"`
    AIContentGenerated  int                    `json:"ai_content_generated"`
    PerformanceMetrics  *PerformanceMetrics    `json:"performance_metrics"`
    UserEngagement      *UserEngagement        `json:"user_engagement"`
    AIEffectiveness     *AIEffectiveness       `json:"ai_effectiveness"`
    CreatedAt           time.Time              `json:"created_at"`
}

type PerformanceMetrics struct {
    AverageLatency      float64 `json:"average_latency"`
    FrameRate           float64 `json:"frame_rate"`
    BandwidthUsage      float64 `json:"bandwidth_usage"`
    ErrorRate           float64 `json:"error_rate"`
    AIResponseTime      float64 `json:"ai_response_time"`
    ContentGenerationTime float64 `json:"content_generation_time"`
}

type UserEngagement struct {
    InteractionRate     float64 `json:"interaction_rate"`
    SessionDuration     float64 `json:"session_duration"`
    ReturnRate          float64 `json:"return_rate"`
    SatisfactionScore   float64 `json:"satisfaction_score"`
    AIInteractionRate   float64 `json:"ai_interaction_rate"`
    ContentCreationRate float64 `json:"content_creation_rate"`
}

type AIEffectiveness struct {
    AvatarResponseQuality    float64 `json:"avatar_response_quality"`
    ContentGenerationQuality float64 `json:"content_generation_quality"`
    VisionAccuracy           float64 `json:"vision_accuracy"`
    TaskCompletionRate       float64 `json:"task_completion_rate"`
    UserSatisfactionWithAI   float64 `json:"user_satisfaction_with_ai"`
}

func NewAnalyticsManager(db *database.DB) *AnalyticsManager {
    return &AnalyticsManager{db: db}
}

func (am *AnalyticsManager) RecordSessionAnalytics(ctx context.Context, sessionID uuid.UUID) (*SessionAnalytics, error) {
    // Gather session data
    session, err := am.db.GetSession(ctx, sessionID)
    if err != nil {
        return nil, err
    }
    
    // Calculate metrics
    analytics := &SessionAnalytics{
        SessionID:    sessionID,
        Duration:     time.Since(session.CreatedAt),
        CreatedAt:    time.Now(),
    }
    
    // Calculate participant metrics
    participants, err := am.db.GetSessionParticipants(ctx, sessionID)
    if err != nil {
        logging.Error("failed to get session participants", map[string]interface{}{
            "session_id": sessionID,
            "error": err.Error(),
        })
    } else {
        analytics.ParticipantCount = len(participants)
    }
    
    // Calculate interaction metrics
    interactions, err := am.db.GetSessionInteractions(ctx, sessionID)
    if err != nil {
        logging.Error("failed to get session interactions", map[string]interface{}{
            "session_id": sessionID,
            "error": err.Error(),
        })
    } else {
        analytics.InteractionCount = len(interactions)
        analytics.LLMInteractions = am.countLLMInteractions(interactions)
    }
    
    // Calculate performance metrics
    analytics.PerformanceMetrics = am.calculatePerformanceMetrics(ctx, sessionID)
    
    // Calculate user engagement metrics
    analytics.UserEngagement = am.calculateUserEngagement(ctx, sessionID, participants, interactions)
    
    // Calculate AI effectiveness metrics
    analytics.AIEffectiveness = am.calculateAIEffectiveness(ctx, sessionID, interactions)
    
    // Store analytics
    err = am.db.CreateSessionAnalytics(ctx, analytics)
    if err != nil {
        logging.Error("failed to store session analytics", map[string]interface{}{
            "session_id": sessionID,
            "error": err.Error(),
        })
        return nil, err
    }
    
    return analytics, nil
}

func (am *AnalyticsManager) calculatePerformanceMetrics(ctx context.Context, sessionID uuid.UUID) *PerformanceMetrics {
    metrics := &PerformanceMetrics{}
    
    // Calculate average latency
    latencies, err := am.db.GetSessionLatencies(ctx, sessionID)
    if err == nil && len(latencies) > 0 {
        var sum float64
        for _, latency := range latencies {
            sum += latency
        }
        metrics.AverageLatency = sum / float64(len(latencies))
    }
    
    // Calculate frame rate
    frameRates, err := am.db.GetSessionFrameRates(ctx, sessionID)
    if err == nil && len(frameRates) > 0 {
        var sum float64
        for _, rate := range frameRates {
            sum += rate
        }
        metrics.FrameRate = sum / float64(len(frameRates))
    }
    
    // Calculate AI response time
    aiResponseTimes, err := am.db.GetAIResponseTimes(ctx, sessionID)
    if err == nil && len(aiResponseTimes) > 0 {
        var sum float64
        for _, responseTime := range aiResponseTimes {
            sum += responseTime
        }
        metrics.AIResponseTime = sum / float64(len(aiResponseTimes))
    }
    
    return metrics
}

func (am *AnalyticsManager) calculateUserEngagement(ctx context.Context, sessionID uuid.UUID, participants []Participant, interactions []Interaction) *UserEngagement {
    engagement := &UserEngagement{}
    
    if len(participants) > 0 {
        // Calculate interaction rate
        totalInteractions := len(interactions)
        engagement.InteractionRate = float64(totalInteractions) / float64(len(participants))
        
        // Calculate AI interaction rate
        aiInteractions := 0
        for _, interaction := range interactions {
            if interaction.Type == "llm" || interaction.Type == "ai_generated" {
                aiInteractions++
            }
        }
        engagement.AIInteractionRate = float64(aiInteractions) / float64(totalInteractions)
    }
    
    // Calculate average session duration
    var totalDuration time.Duration
    for _, participant := range participants {
        if !participant.LeftAt.IsZero() {
            totalDuration += participant.LeftAt.Sub(participant.JoinedAt)
        }
    }
    if len(participants) > 0 {
        engagement.SessionDuration = totalDuration.Seconds() / float64(len(participants))
    }
    
    return engagement
}

func (am *AnalyticsManager) calculateAIEffectiveness(ctx context.Context, sessionID uuid.UUID, interactions []Interaction) *AIEffectiveness {
    effectiveness := &AIEffectiveness{}
    
    // Calculate avatar response quality
    avatarResponses, err := am.db.GetAvatarResponses(ctx, sessionID)
    if err == nil && len(avatarResponses) > 0 {
        var totalQuality float64
        for _, response := range avatarResponses {
            totalQuality += response.Quality
        }
        effectiveness.AvatarResponseQuality = totalQuality / float64(len(avatarResponses))
    }
    
    // Calculate content generation quality
    generatedContent, err := am.db.GetGeneratedContent(ctx, sessionID)
    if err == nil && len(generatedContent) > 0 {
        var totalQuality float64
        for _, content := range generatedContent {
            totalQuality += content.Quality
        }
        effectiveness.ContentGenerationQuality = totalQuality / float64(len(generatedContent))
    }
    
    // Calculate task completion rate
    tasks, err := am.db.GetSessionTasks(ctx, sessionID)
    if err == nil && len(tasks) > 0 {
        completedTasks := 0
        for _, task := range tasks {
            if task.Status == "completed" {
                completedTasks++
            }
        }
        effectiveness.TaskCompletionRate = float64(completedTasks) / float64(len(tasks))
    }
    
    return effectiveness
}
```

## Implementation Timeline

### Month 1: LLM Integration
- [ ] Week 1: OpenAI API integration and prompt engineering
- [ ] Week 2: LLM avatar system implementation
- [ ] Week 3: Natural language command processing
- [ ] Week 4: Avatar behavior and memory systems

### Month 2: AI Enhancement
- [ ] Week 1: Computer vision integration
- [ ] Week 2: AI content generation pipeline
- [ ] Week 3: Analytics and monitoring systems
- [ ] Week 4: Testing and optimization

## Success Criteria

### Technical Metrics
- [ ] LLM avatars respond within 2 seconds
- [ ] Computer vision analyzes scenes with 90% accuracy
- [ ] AI content generation produces high-quality results
- [ ] Analytics provides comprehensive insights
- [ ] Natural language processing with 95% intent recognition

### Quality Metrics
- [ ] 100% API test coverage for all 20 new endpoints
- [ ] AI responses maintain consistent personality
- [ ] Generated content meets quality standards
- [ ] Performance monitoring shows optimal AI usage
- [ ] User satisfaction with AI features > 4.5/5

### Business Metrics
- [ ] AI avatars increase user engagement by 60%
- [ ] Content generation reduces creation time by 80%
- [ ] Analytics provide actionable insights
- [ ] Natural language interface improves accessibility
- [ ] Platform supports 1000+ simultaneous AI interactions

## Risk Mitigation

### Technical Risks
- **API Rate Limits**: Implement intelligent caching and batching
- **AI Response Quality**: Use fine-tuning and prompt optimization
- **Performance Impact**: Optimize AI processing and use edge computing
- **Data Privacy**: Implement privacy-preserving AI techniques

### Business Risks
- **AI Costs**: Monitor usage and implement cost controls
- **User Acceptance**: Gradual rollout with user feedback
- **Ethical Concerns**: Implement AI ethics guidelines
- **Regulatory Compliance**: Ensure AI compliance with regulations

## Deliverables

### Code Deliverables
- [ ] LLM avatar system with personality and memory
- [ ] Computer vision for 3D scene analysis
- [ ] AI content generation pipeline
- [ ] Natural language interface
- [ ] Comprehensive analytics system
- [ ] 20 new API endpoints with documentation

### Documentation Deliverables
- [ ] AI integration guide
- [ ] LLM avatar development guide
- [ ] Content generation documentation
- [ ] Analytics and monitoring guide
- [ ] Natural language interface guide

### Testing Deliverables
- [ ] AI response quality tests
- [ ] Content generation validation tests
- [ ] Performance tests for AI systems
- [ ] User experience tests
- [ ] Comprehensive integration tests

This completes the detailed Phase 3 implementation plan. The AI integration will transform HD1 into an intelligent, responsive platform with advanced AI capabilities for natural interaction and content generation.